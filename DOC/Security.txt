= Protected mode Ring reference =

				ring 0: pmode, paging
				ring 1: scheduler, kernel api
				ring 2: drivers (IO)
				ring 3: unprivileged

- Ring1:
  Both scheduler and kernel api have IOPL but do not use it. Ring 0 code is only
  executed through the kernel API, if needed.

- Ring 2:
  It seems to make sense to have drivers have less privilege than the
  kernel api itself, since drivers can be third party, since they are
  unprivileged due to not being ring0, yet have increased privilege only on
  the IO level aswell as memory access.
  The IO permission table can be configured per interrupt handler/driver,
  so that they only have access to the device and perhaps PCI. Ring 1 code does
  have access to all ports. A GPF handler can be installed to be handled
  by ring 1 code when ring2 attempts access, and this can either result in the
  driver being rejected, or masquerading the IO operations, such as PCI reads.
  It is however easier to provide a PCI api method.

- Ring 3:
  this code has no access to privileged pages and no IO access.

Drivers can reside in ring3, but then they have no access to ring 1 memory,
unless CR3 is switched out on IRQ and ring 2 API calls.

Ring3 processes have no access to ring2.

		CS DPL	Memory		DS DPL	IO
	Ring 0:	1 C	System		0 C	All
	Ring 1: 3 C	System		1 C	All
	Ring 2:	1 	System		2 NC	Partial
	Ring 3: 1 NC	Unprivileged	3 N/A	No

CS DPL: code able to execute privilege level change.
Ring 0 can only be called from ring 0 or ring 1.
Ring 1 can be called from 0-3.
Ring 2 can be called from ring 1 (so not other drivers in ring 2 or user code).
Ring 3 can only be called from ring 1 (scheduler, kernel api).

DS CPL:
Ring 0 is only accessible from ring 0.
Ring 1 only from ring 1, yet ring 0 is root, and thus always has access.
Ring 2 is only accessible from ring 2. Thus even the kernel api has to
go through elevation to access ring2 data through a ring2 selector. However,
the kernel api can be configured to have access to that memory. Ring 0
has unlimited data access.
Ring 3 is probably conforming ring 3, otherwise drivers cannot write in task
buffers.

= Example =

Drivers must have access to all task buffers (disk read for instance).
Unprivileged task (3) calls the kernel API (1) to read from the file system.
The kernel API executes the driver (2), who reads into the task (3) memory.

Task buffers can be separated from task memory, since the kernel provides
all the memory addresses. The heap therefore, is distinct from the task image.
The driver API can be implemented as task gates, which do a stack and paging
switch. This paging switch allows to map only the task's heap into the driver
memory space. Further, there can be a kernel api to allocate buffers, besides
malloc. It would use the paging memory allocation system, instead. This way,
the kernel will know what memory addresses are buffers to be used for devices.

Another approach is to have the drivers use kernel memory. This way, a region
of memory can be mapped. It would have to then copy the data to the task space,
since it can use malloced memory, which is generally not page aligned.

NOTE: the scheduler must run at CPL0 if it switches out CR3, unless it uses
hardware task switching. In this case, the schedule_isr becomes a task gate.
It is not called as a task, because CR3 doesn't need switching unless another
task is scheduled to run. But it can set up the task switch TSS to be used
and jump through the hoop, avoiding iret. In this case it would not
pushall/popall, but rep movsd twice, not to mention the hardware overhead
(unless there is smart management of a dynamic array of TSS, where the most
run tasks' TSS is kept intact - TSS caching, as it were.)

= Paging =

The paging setup is done in CPL0, after which certain tables can be made
writable to the whole system (ring 0-2). Starting out with no page tables
writable, ring 1 code can ask to allocate a PDE or PTE, to be mapped in
the ring 1 memory space, which makes all memory accessible to it. It can
simply flush the paging TLB by doing a yield, or, since it is at the same
ring as the scheduler, can be given a task gate for this purpose.
There is no way using paging to indicate a difference between rings 0, 1 and2.
If they added just one more bit, it would have been.
To make ring 1 and 2 work, then, selectors must be used.
Allocating pages from end of memory down allows to limit the selectors for
rings 1 and 2. (Ring 3 limitation can be done using paging). This makes
for a reserved region of memory at the end of memory. This can be done in
layers, each lower ring number having further access; for example:

	Ring 0:	[0-4Gb]
	Ring 1: [2Mb-3Gb]
	Ring 2: [3Mb-2Gb]
	Ring 3: [4Mb-1Gb].

The kernel starts at 1Mb. The symbols 2Mb indicate the start of ring 0
code called by ring 1, followed by ring 1 code. 3Mb/4Mb idem.
This allows to separate code access. For data access on such granularity,
either each ring must be relocated independently, and thus far calls
are needed (allowing for stack switching aswell), or it must be done
on a paging level. This last approach has the downside, that the scheduler
at ring 1, must have control over task page memory. This cannot be directly
limited, since having a PDE or PTE at it's disposal, it can access any
memory. Therefore, this code must offer the table for validation before
the kernel enables it. This allows ring0 to provide both code and data
access protection of the kernel (as long as the ringed code and data
are page aligned).


= Driver isolation =

In order to support driver isolation, interrupt handlers should be created
like tasks (either via task gates or via the scheduler).

When created _as_ tasks, the `irq_isr` method would instead hold a list of
task IDs (PIDs) rather than the irq handler. These tasks would auto-suspend
themselves at their start address after handling the job. A second way is
for them to yield and check some variable.
Lastly (and most complexly), they can yield on a semaphore. The kernel or
irq_isr method will hold a write lock on the semaphore, and will release it
when an interrupt occurs, so as to enable multiple parallel read locks to
succeed, one for each task. 
The write lock must then be re-acquired as soon as all IRQ handlers have
finished, which would require some special scheduler code.
Task scheduling code already takes care of updating the page table register,
which should map the task's heap for buffers, some read-only kernel space
(class definitions etc), the IO ports in the current task descriptor (there
should be one in use all the time).

For now, `irq_isr` is executing at CPL0 and iteratively calling all registered
IRQ handlers for the raised IRQ. The simplest solution is to have it decrease
the CPL to 2 during the handler iteration. It should however resume CPL0
when returning from the interrupt handler, for two reasons: First, the `iret`
takes care of restoring the userspace stack, if any, which it does by checking
the CPL/DPL; Secondly, all interrupts, upon completion, invoke the scheduler,
which expects to be run at CPL0.

== Malloc, .tls ==

Malloc code would need to have the base pointers to it's structures placed
in a .tls (task local storage segment) to be able to re-use malloc
functionality with separate heaps for different tasks.

When not wanting to use a task for each interrupt,
the `irq_isr` can take on a scheduling role by updating the GDTR,
(and possibly the IDTR, allow to remap interrupt handling during an IRQ).

It is possibly far faster to only remap part of the page table.
A region should be reserved/calculated relative to kernel space and it's
basic heap needs (we don't want the kernel heap/stack to run into this space)
that serves as the basis of mapping task heap space (within the range of it's
ds/es). Typically IO drivers consume at a maximum 64kb (due to bus mastering
limitations), which is 16 pages. 16 `INVLPG` instructions is likely faster
than switching out `CR3`.
However it would be better if the kernel could have a view as it has now,
namely, a flat view of at least kernel applications.

== Driver memory allocation tracking ==

A first step would be to find all mallocs in drivers and replace them
with a new function `dev_malloc`. There is already caller tracking code,
and recognizing the caller signature (offset within dev code), malloc could
use another reserved region. It would malloc a large region, and initialize it's
beginning with another malloc structure. All malloc calls from dev will
then be proxied to a malloc call using the beginning of the dev-mallocced
buffer. This buffer would be so initialized as to treat the space after
the header as the heap to manage. In this way, malloc becomes a tree.
It would allow for more freedom than merely remapping .tls.

== Malloc updates ([src:kernel/lib/mem.s]) ==

When not using only .tls based malloc space separation, a more
general approach is to modify malloc to take one more memory dereference
(remove references to static variables). This will make it possible to
multiple memory regions be manageable by the memory management code.

Then, dev_malloc would serve as a template for a recursive malloc,
restricted to merely allow to compactly group page-aligned regions.

	.data
		dev_malloc_base$: .long 0
		dev_malloc_kernel_ptr$: .long 0	 # [mem_kernel] pool
		dev_malloc_kernel_size$: .long 0 # [mem_kernel] allocated size
	dev_malloc:
		push	esi
		mov	esi, [dev_malloc_base$]
		or	esi,esi
		jz	1f
	2:
		call	malloc_internal$
		jc	3f

	9:	pop	esi
		ret

	1:	push	eax
		add	eax, 4095 + MEM_STRUCT_SIZE
		and	eax, ~4095
		mov	[dev_malloc_kernel_size$], eax
		push	eax
		call	mallocz
	4:
		# remember original:
		mov	[dev_malloc_kernel_ptr$], eax
		# find first page alignment:
		mov	esi, eax
		GDT_GET_BASE eax, ds
		add	esi, eax
		add	esi, MEM_STRUCT_SIZE + 4096
		and	esi, ~4095
		sub	esi, eax
		mov	[dev_malloc_base$], esi

		lea	eax, [esi + MEM_STRUCT_SIZE]
		mov	[esi + heap_base], eax
		pop	[esi + heap_size]
		mov	[esi + ll_first], dword ptr -1
		mov	[esi + ll_last], dword ptr -1
		mov	[esi + mem_handle_ll_(fa|fs|fh) + ll_(first|last)], -1
		pop	eax
		jmp	2b
	
	3:	push_	edx eax
		mov	edx, [dev_malloc_kernel_size$]
		add	edx, eax
		call	mreallocz
		jc	1f
		mov	[dev_malloc_kernel_size$], edx
	1:	pop_	eax edx
		jc	9b
		jmp	4b

See the MEM_FEATURE_STRUCT code in [src:kernel/lib/mem.s]. This code
affects `malloc_internal$`, which manages the heap reservation.
This is sufficient to have separate handle structures, however,
the heap addresses are not localized/dynamic, which is why all `[mem_*]`
(except `[mem_kernel]`) are fetched using pointer indirection.

We can now either allocate pages from the high memory page pool,
and map these anywhere,
or we can allocate page aligned memory and simply change the permissions
on certain regions of the kernel heap space, knowing that
1) the kernel doesn't know or need to know what is inside these regions
 (except as it gets passed pointers to it)
2) the region itself is managed by special code.

The `dev_malloc` sample above maintains a compact mapping by using
mreallocz to extend the size of the chunk of memory allocated from
the kernel heap. This will change all the previously allocated
memory pointers. Since driver code is interrupt based, such resizes
can be safely done outside of driver interrupt handlers. Typically
memory allocation is only done during device initialisation and
can be restricted. Even though, any mrealloc will change previously
stored pointers, internal consistency is guaranteed, as the handle
structure of the nested dev memory pool is untouched. All that then
remains is to page map the dev_malloc_base to a fixed page address.

Furthermore, buffer memory to be passed as hardware addresses should
be handled by a separate routine. DMA has certain restrictions, as do
older ISA cards, which require buffers to be in low memory.

The page pool should scale; it is basically the 4Gb region (above 1mb or so)
that can be mapped in any sequence. As such it is a dynamic array, and it is
compact aswell, clustering in high memory. 

The kernel could first make the distinction of DMA memory, knowing that
this is hardware (CPL2) accessed, and can be in 1 pool.
Then, the pool should be contiguous regions separated by drivers,
so that they cannot influence other buffers.

The kernel network code should also be custom mapped to include any
NIC device buffer space. Bookeeping data mallocced by the driver
is generally not referenced (passed as pointers) so the driver heap
can remain hidden to the netq daemon.

The mrealloc scheme above will remain problematic unless some paging solution
is implemented. This defeats the purpose of having a single flat view for
the kernel: even though it has access to all pages through identity mapping,
the sequence and actual mapped addresses differ.
A space can be reserved for CPL2 mapping; each driver will then use the
same memory address as a heap. The kernel then cannot access data from
two device drivers simultaneously. This should however not pose a problem,
since kernel calls done from a device driver interrupt or API handler
will have the proper context mapped.
